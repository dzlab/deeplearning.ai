{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "315c6587",
   "metadata": {},
   "source": [
    "# Lesson 4: Keeping LLMs Private"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8019c59",
   "metadata": {},
   "source": [
    "Welcome to Lesson 4!\n",
    "\n",
    "To access the `requirements.txt` and `utils` files for this course, go to `File` and click `Open`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a21ffb9",
   "metadata": {},
   "source": [
    "#### 1. Import packages and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7795ec8f-d428-4596-9038-4ae14edc58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from utils.utils import get_config, visualize_results, print_config\n",
    "from utils.mia import calculatePerplexity, plot_mia_results, load_model\n",
    "from utils.mia import get_evaluation_data, MIA_test, load_extractions\n",
    "from utils.mia import evaluate_data, analyse\n",
    "from utils.LLM import LLM_cen, LLM_fl, LLM_pretrained\n",
    "from utils.LLM import get_fireworks_api_key,load_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25908b90",
   "metadata": {},
   "source": [
    "#### 2. Ask the 7B Mistral LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e255cf7e",
   "metadata": {},
   "source": [
    "* Load the 7B Mistral LLM model that was centrally fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3c84cd-0a68-4b3b-a8d3-e596dc92b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cen_llm = LLM_cen()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c85b9",
   "metadata": {},
   "source": [
    "* Test with different prompt.\n",
    "\n",
    "The instructor tests, while explaining, with \n",
    "\n",
    "```\n",
    "prompt = \"Peter W\"\n",
    "```\n",
    "\n",
    "```\n",
    "prompt = \"email address:\"\n",
    "```\n",
    "\n",
    "```\n",
    "prompt = \"Peter W\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7709111d-a535-4daa-8854-1bffc43bbbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different prompts\n",
    "prompt = \"Peter W\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad81d7-789b-4457-8e2f-6b152c77152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the prompt and its response\n",
    "cen_llm.eval(prompt)\n",
    "output = cen_llm.get_response()\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Response: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6855808",
   "metadata": {},
   "source": [
    "#### 3. Calculate 'perplexity'\n",
    "\n",
    "Perplexity measures how well the LLM can predict those sequences of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e3d6a-aad5-440f-a7af-51cfe85e2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prompt: {output}\")\n",
    "\n",
    "# Use secondary attribute set to True\n",
    "cen_llm.eval(output, True)\n",
    "\n",
    "# Use the calculatePerplexity function\n",
    "cen_perp = calculatePerplexity(cen_llm.get_response_raw())\n",
    "print(f\"Perplexity: {cen_perp:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ef1f1",
   "metadata": {},
   "source": [
    "* Calculate perplexity with other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff267c89-3923-4f37-bb25-1522f810ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data found on the web \n",
    "prompt_text = \"With the cold weather setting in and the \" \\\n",
    "            \"stress of the Christmas holiday approaching\"\n",
    "\n",
    "# Use secondary attribute set to True\n",
    "cen_llm.eval(prompt_text, True)\n",
    "\n",
    "cen_perp = calculatePerplexity(cen_llm.get_response_raw())\n",
    "print(f\"Perplexity (in-dataset): {cen_perp:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a5172-b08e-406f-9758-c9c076567d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text article from the Guardian\n",
    "prompt_text = f\"No evidence foreign students are abusing \" \\\n",
    "                \"UK graduate visas, review finds\"\n",
    "cen_llm.eval(prompt_text, True)\n",
    "cen_perp = calculatePerplexity(cen_llm.get_response_raw())\n",
    "print(f\"Perplexity (out-of-dataset): {cen_perp:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3355637",
   "metadata": {},
   "source": [
    "#### 4. Load the LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d25af8",
   "metadata": {},
   "source": [
    "* Load the centrally fine-tuned LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01838c-6b58-4a7f-9152-0bd4bea894c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_llm = LLM_pretrained()\n",
    "\n",
    "prompt_text = \"With which class of antimicrobials is Aztreonam \"\\\n",
    "              \"particularly synergistic?\",\n",
    "cen_llm.eval(prompt_text, True)\n",
    "cen_perp = calculatePerplexity(cen_llm.get_response_raw())\n",
    "\n",
    "pre_llm.eval(prompt_text, True)\n",
    "pre_perp = calculatePerplexity(pre_llm.get_response_raw())\n",
    "\n",
    "print(f\"Normalised perplexity: {cen_perp/pre_perp:.3f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f7dddb",
   "metadata": {},
   "source": [
    "* Load FL (Federated Learning) with LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03adf867-74e6-49ab-82d1-3636e3b1d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_llm = LLM_fl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78c1cb5-52f9-48b0-ab78-f2991074fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    \"Among all branchial arches, which arch gives rise \"\\\n",
    "    \"to the stylohyoid muscle and stylohyoid ligament?\",\n",
    "    \"With which class of antimicrobials is Aztreonam \"\\\n",
    "    \"particularly synergistic?\",\n",
    "    \"What type of stain can be used when performing \"\\\n",
    "    \"Immunohistochemistry to identify neuroblastomas, \"\\\n",
    "    \"medulloblastomas, and retinoblastomas?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675794e-1198-4bc7-b94f-1310b51ca480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print analysis when using Centrally fine-tuned model vs Federated + DP fine-tuned model\n",
    "print(\"Analysis Centrally Finetuned model:\")\n",
    "MIA_test(cen_llm, prompt_list)\n",
    "print(\"Analysis Federated + DP finetuned model:\")\n",
    "MIA_test(fl_llm, prompt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd9e24f",
   "metadata": {},
   "source": [
    "* Try with a larger experiment (set new confirguration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2746ccd0-bb15-4c8f-8aeb-4c9bbd99d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set new configuration\n",
    "mia_cfg = get_config(\"mia\")\n",
    "\n",
    "print_config(mia_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57e6806",
   "metadata": {},
   "source": [
    "**Note**: You will be prompted to use the custom code. Please type \"y\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c66f1a-82fa-4401-80e8-200d05df250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the outputs' models using the large dataset\n",
    "(fl_fine_tuned_model,\n",
    " cen_fine_tuned_model,\n",
    " pre_trained_model, tokenizer) = load_model(mia_cfg)\n",
    "\n",
    "data = get_evaluation_data(mia_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb9889",
   "metadata": {},
   "source": [
    "* Print the ROC.\n",
    "\n",
    "To analyse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee023f99-1e35-4b6a-852a-71a6965c31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mia_results(data,\n",
    "    fl_fine_tuned_model,\n",
    "    cen_fine_tuned_model,\n",
    "    pre_trained_model,\n",
    "    tokenizer,\n",
    "    mia_cfg.key_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7f63f",
   "metadata": {},
   "source": [
    "* Explore more examples when using 7B Mistral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3858d2-3cea-4bd3-b4dd-0a2a12e185aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction = load_extractions(mia_cfg.key_name)\n",
    "extraction.eval()\n",
    "extraction.show('url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7082b7d-8958-42be-9b4d-278f38bfdab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction.show('email')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
