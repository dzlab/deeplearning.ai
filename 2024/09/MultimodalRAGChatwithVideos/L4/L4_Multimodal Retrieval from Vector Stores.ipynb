{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ed2fd2",
   "metadata": {},
   "source": [
    "# Lesson 4: Multimodal Retrieval from Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa86cd1",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>Note:</b>\n",
    "We'll use downloaded videos, extracted frames, transcription,\n",
    "<br>and generated captions from Lesson 3. \n",
    "<br>\n",
    "These data is already included in the symlinked folder `shared_folder`.\n",
    "<br>\n",
    "If you haven't already practiced the lesson 3, we pre-populated them.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09debe28",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a4a8f3-77fc-40fd-a5ab-1e4c98cad4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mm_rag.embeddings.bridgetower_embeddings import (\n",
    "    BridgeTowerEmbeddings\n",
    ")\n",
    "from mm_rag.vectorstores.multimodal_lancedb import MultimodalLanceDB\n",
    "import lancedb\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from utils import load_json_file\n",
    "from utils import display_retrieved_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ab9b0d",
   "metadata": {},
   "source": [
    "### Setup LanceDB vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efeb762-2ea9-4073-b2eb-7b5acdecd0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare host file\n",
    "LANCEDB_HOST_FILE = \"./shared_data/.lancedb\"\n",
    "# declare table name\n",
    "TBL_NAME = \"test_tbl\"\n",
    "# initialize vectorstore\n",
    "db = lancedb.connect(LANCEDB_HOST_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf0cc7",
   "metadata": {},
   "source": [
    "## Ingest Video Corpuses to LanceDB Using Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a475d7",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7287d089-20f2-43c0-829c-b260e2f78c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata files\n",
    "vid1_metadata_path = './shared_data/videos/video1/metadatas.json'\n",
    "vid2_metadata_path = './shared_data/videos/video2/metadatas.json'\n",
    "vid1_metadata = load_json_file(vid1_metadata_path)\n",
    "vid2_metadata = load_json_file(vid2_metadata_path)\n",
    "\n",
    "# collect transcripts and image paths\n",
    "vid1_trans = [vid['transcript'] for vid in vid1_metadata]\n",
    "vid1_img_path = [vid['extracted_frame_path'] for vid in vid1_metadata]\n",
    "\n",
    "vid2_trans = [vid['transcript'] for vid in vid2_metadata]\n",
    "vid2_img_path = [vid['extracted_frame_path'] for vid in vid2_metadata]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a558bf7",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>Notes:</b>\n",
    "<br>\n",
    "- We observe that the transcripts of frames extracted from video1 are usually fragmented and even an incomplete sentence. E.g., <i>four more was just icing on the cake for a</i>. Thus, such transcripts are not meaningful and are not helpful for retrieval. In addition, a long transcript that includes many information is also not helpful in retrieval. A naive solution to this issue is to augment such a transcript with the transcripts of n neighboring frames. It is advised that we should pick an individual n for each video such that the updated transcripts say one or two meaningful facts.\n",
    "<br>\n",
    "- It is ok to have updated transcripts of neighboring frames overlapped with each other.\n",
    "<br>\n",
    "- Changing the transcriptions which will be ingested into vector store along with their corresponding frames will affect directly the performance. It is advised that one needs to do diligent to experiment with one's data to get the best performance.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0f80a-7810-49a9-98c6-3d8b88a74049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for video1, we pick n = 7\n",
    "n = 7\n",
    "updated_vid1_trans = [\n",
    " ' '.join(vid1_trans[i-int(n/2) : i+int(n/2)]) if i-int(n/2) >= 0 else\n",
    " ' '.join(vid1_trans[0 : i + int(n/2)]) for i in range(len(vid1_trans))\n",
    "]\n",
    "\n",
    "# also need to update the updated transcripts in metadata\n",
    "for i in range(len(updated_vid1_trans)):\n",
    "    vid1_metadata[i]['transcript'] = updated_vid1_trans[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e757c0-384c-4611-8444-a09da49fe06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'A transcript example before update:\\n\"{vid1_trans[6]}\"')\n",
    "print()\n",
    "print(f'After update:\\n\"{updated_vid1_trans[6]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc4c840",
   "metadata": {},
   "source": [
    "### Ingest Data to LanceDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53608479-38b6-4aad-bcf3-44c83b0a5d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an BridgeTower embedder \n",
    "embedder = BridgeTowerEmbeddings()\n",
    "\n",
    "\n",
    "# you can pass in mode=\"append\" \n",
    "# to add more entries to the vector store\n",
    "# in case you want to start with a fresh vector store,\n",
    "# you can pass in mode=\"overwrite\" instead \n",
    "\n",
    "_ = MultimodalLanceDB.from_text_image_pairs(\n",
    "    texts=updated_vid1_trans+vid2_trans,\n",
    "    image_paths=vid1_img_path+vid2_img_path,\n",
    "    embedding=embedder,\n",
    "    metadatas=vid1_metadata+vid2_metadata,\n",
    "    connection=db,\n",
    "    table_name=TBL_NAME,\n",
    "    mode=\"overwrite\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e3a55f",
   "metadata": {},
   "source": [
    "## Multimodal Retrieval Using Langchain\n",
    "\n",
    "### Helper function\n",
    "\n",
    "A helper function `display_retrieved_results` in `utils.py` to display retrieved results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a73b219",
   "metadata": {},
   "source": [
    "### Create Connection to a Table in LanceDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c259cf-20e0-4868-bb30-ce0d304b1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a connection to table TBL_NAME\n",
    "tbl = db.open_table(TBL_NAME)\n",
    "\n",
    "print(f\"There are {tbl.to_pandas().shape[0]} rows in the table\")\n",
    "# display the first 3 rows of the table\n",
    "tbl.to_pandas()[['text', 'image_path']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bffda8b",
   "metadata": {},
   "source": [
    "### Retrieval from LanceDB Vector Store using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9000c8-da1d-484c-bedc-a7527e3b2f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a LanceDB vector store \n",
    "vectorstore = MultimodalLanceDB(\n",
    "    uri=LANCEDB_HOST_FILE, \n",
    "    embedding=embedder, \n",
    "    table_name=TBL_NAME)\n",
    "\n",
    "# creating a retriever for the vector store\n",
    "# search_type=\"similarity\" \n",
    "#  declares that the type of search that the Retriever should perform \n",
    "#  is similarity search\n",
    "# search_kwargs={\"k\": 1} means returning top-1 most similar document\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type='similarity', \n",
    "    search_kwargs={\"k\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1121a4-89f4-4aa1-bb61-1a310925e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"a toddler and an adult\"\n",
    "results = retriever.invoke(query1)\n",
    "display_retrieved_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afadbb27-c716-4f0b-9d28-2e904fff8ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask to return top 3 most similar documents\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type='similarity', \n",
    "    search_kwargs={\"k\": 3})\n",
    "results = retriever.invoke(query1)\n",
    "display_retrieved_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5600bc0e-f792-4bbe-8e5b-03b12bbea668",
   "metadata": {},
   "source": [
    "### More Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10651e8-cc56-45e6-8153-39ae2bfc93a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type='similarity', \n",
    "    search_kwargs={\"k\": 1})\n",
    "query2 = (\n",
    "        \"an astronaut's spacewalk \"\n",
    "        \"with an amazing view of the earth from space behind\"\n",
    ")\n",
    "results2 = retriever.invoke(query2)\n",
    "display_retrieved_results(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8aa881-3b62-49aa-9f05-1ce9d4180600",
   "metadata": {},
   "outputs": [],
   "source": [
    "query3 = \"a group of astronauts\"\n",
    "results3 = retriever.invoke(query3)\n",
    "display_retrieved_results(results3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493c568-0038-46b1-bd0d-e22792f86358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
