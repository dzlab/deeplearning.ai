{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a67eaf44",
   "metadata": {},
   "source": [
    "# Lesson 2: Multimodal Embeddings\n",
    "\n",
    "## Multimodal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43370a-31c9-4f76-9b40-28293ca5946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# You can use your own uploaded images and captions. \n",
    "# You will be responsible for the legal use of images that \n",
    "#  you are going to use.\n",
    "\n",
    "url1='http://farm3.staticflickr.com/2519/4126738647_cc436c111b_z.jpg'\n",
    "cap1='A motorcycle sits parked across from a herd of livestock'\n",
    "\n",
    "url2='http://farm3.staticflickr.com/2046/2003879022_1b4b466d1d_z.jpg'\n",
    "cap2='Motorcycle on platform to be worked on in garage'\n",
    "\n",
    "url3='http://farm1.staticflickr.com/133/356148800_9bf03b6116_z.jpg'\n",
    "cap3='a cat laying down stretched out near a laptop'\n",
    "\n",
    "img1 = {\n",
    "  'flickr_url': url1,\n",
    "  'caption': cap1,\n",
    "  'image_path' : './shared_data/motorcycle_1.jpg'\n",
    "}\n",
    "\n",
    "img2 = {\n",
    "    'flickr_url': url2,\n",
    "    'caption': cap2,\n",
    "    'image_path' : './shared_data/motorcycle_2.jpg'\n",
    "}\n",
    "\n",
    "img3 = {\n",
    "    'flickr_url' : url3,\n",
    "    'caption': cap3,\n",
    "    'image_path' : './shared_data/cat_1.jpg'\n",
    "}\n",
    "\n",
    "# download images\n",
    "imgs = [img1, img2, img3]\n",
    "for img in imgs:\n",
    "    data = requests.get(img['flickr_url']).content\n",
    "    with open(img['image_path'], 'wb') as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d606f5-86af-4ebe-a4a7-c59292818022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "for img in [img1, img2, img3]:\n",
    "    image = Image.open(img['image_path'])\n",
    "    caption = img['caption']\n",
    "    display(image)\n",
    "    display(caption)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175767ae",
   "metadata": {},
   "source": [
    "## BridgeTower Embedding Computation\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e66ff2-d8c9-4d55-93bd-eebb43b353b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import cv2\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils import encode_image\n",
    "from utils import bt_embedding_from_prediction_guard as bt_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d175cd6",
   "metadata": {},
   "source": [
    "### Compute Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6482ced-e21e-4cdd-922a-1878ed0032a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for img in [img1, img2, img3]:\n",
    "    img_path = img['image_path']\n",
    "    caption = img['caption']\n",
    "    base64_img = encode_image(img_path)\n",
    "    embedding = bt_embeddings(caption, base64_img)\n",
    "    embeddings.append(embedding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9981c5b6-3815-4724-b62a-02af1f812955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each image-text pair is now converted into multimodal \n",
    "# embedding vector which has dimensions of 512.\n",
    "\n",
    "print(len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c38c384",
   "metadata": {},
   "source": [
    "### Cosine Similarity Between Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e7c6d-efd8-440f-8937-a9fc4c04e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    similarity = np.dot(vec1,vec2)/(norm(vec1)*norm(vec2))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8ecd1-6356-49bc-b22e-55ea138342de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_embed = np.array(embeddings[0])\n",
    "ex2_embed = np.array(embeddings[1])\n",
    "ex3_embed = np.array(embeddings[2])\n",
    "sim_ex1_ex2 = cosine_similarity(ex1_embed, ex2_embed)\n",
    "sim_ex1_ex3 = cosine_similarity(ex1_embed, ex3_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28724968-0ba3-4eee-b308-8f334edce9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cosine similarity between ex1_embeded and ex2_embeded is:\")\n",
    "display(sim_ex1_ex2)\n",
    "print(\"Cosine similarity between ex1_embeded and ex3_embeded is:\")\n",
    "display(sim_ex1_ex3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c46d8d8",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>Note:</b>\n",
    "As expected, since the 1st and the 2nd images all depict motorcycles \n",
    "<br>\n",
    "and the third image depicts a cat, which is different from motorcycle, \n",
    "<br>\n",
    "the Cosine similarity between the 1st and the 2nd examples' embeddings\n",
    "<br> \n",
    "is greater than that between the 1st and the 3rd examples' embeddings.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ddc4b",
   "metadata": {},
   "source": [
    "### Euclidean Distance Between Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd2c879-948c-4689-963f-9192d07d1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_ex1_ex2 = cv2.norm(ex1_embed, ex2_embed, cv2.NORM_L2)\n",
    "dist_ex1_ex3 = cv2.norm(ex1_embed, ex3_embed, cv2.NORM_L2)\n",
    "\n",
    "print(\"Euclidean distance between ex1_embeded and ex2_embeded is:\")\n",
    "display(dist_ex1_ex2)\n",
    "\n",
    "print(\"Euclidean distance between ex1_embeded and ex3_embeded is:\")\n",
    "display(dist_ex1_ex3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c181c0",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>Note:</b>\n",
    "As expected, since the 1st and the 2nd images all depict motorcycles \n",
    "<br>\n",
    "and the third image depicts a cat, which is different from motorcycle, \n",
    "<br>\n",
    "the Euclidean distance between the 1st and the 2nd examples' embeddings\n",
    "<br> \n",
    "is smaller than that between the 1st and the 3rd examples' embeddings.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e961f9",
   "metadata": {},
   "source": [
    "### Visualizing High-dimensional Data with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e460ef50-c597-402d-bd13-28b397d7a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import prepare_dataset_for_umap_visualization as data_prep\n",
    "\n",
    "# prepare image_text pairs \n",
    "\n",
    "# for the first 50 data of Huggingface dataset \n",
    "#  \"yashikota/cat-image-dataset\"\n",
    "cat_img_txt_pairs = data_prep(\"yashikota/cat-image-dataset\", \n",
    "                             \"cat\", test_size=50)\n",
    "\n",
    "# for the first 50 data of Huggingface dataset \n",
    "#  \"tanganke/stanford_cars\"\n",
    "car_img_txt_pairs = data_prep(\"tanganke/stanford_cars\", \n",
    "                             \"car\", test_size=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1922f658-b997-4625-8edd-70ed8160d928",
   "metadata": {},
   "source": [
    "> Note: your images may differ from those seen in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f5e5b-fbff-4b67-a460-84237307c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display an example of a cat image-text pair data\n",
    "display(cat_img_txt_pairs[0]['caption'])\n",
    "display(cat_img_txt_pairs[0]['pil_img'])\n",
    "\n",
    "# display an example of a car image-text pair data\n",
    "display(car_img_txt_pairs[0]['caption'])\n",
    "display(car_img_txt_pairs[0]['pil_img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d103a-e18e-4600-87c2-4308e00c1430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute BridgeTower embeddings for cat image-text pairs\n",
    "cat_embeddings = []\n",
    "for img_txt_pair in tqdm(\n",
    "                        cat_img_txt_pairs, \n",
    "                        total=len(cat_img_txt_pairs)\n",
    "                    ):\n",
    "    pil_img = img_txt_pair['pil_img']\n",
    "    caption = img_txt_pair['caption']\n",
    "    base64_img = encode_image(pil_img)\n",
    "    embedding = bt_embeddings(caption, base64_img)\n",
    "    cat_embeddings.append(embedding)\n",
    "\n",
    "# compute BridgeTower embeddings for car image-text pairs\n",
    "car_embeddings = []\n",
    "for img_txt_pair in tqdm(\n",
    "                        car_img_txt_pairs, \n",
    "                        total=len(car_img_txt_pairs)\n",
    "                    ):\n",
    "    pil_img = img_txt_pair['pil_img']\n",
    "    caption = img_txt_pair['caption']\n",
    "    base64_img = encode_image(pil_img)\n",
    "    embedding = bt_embeddings(caption, base64_img)\n",
    "    car_embeddings.append(embedding)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94483643-742f-4504-98ea-c00c4eb08f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function transforms high-dimension vectors to 2D vectors using UMAP\n",
    "def dimensionality_reduction(embed_arr, label):\n",
    "    X_scaled = MinMaxScaler().fit_transform(embed_arr)\n",
    "    print(X_scaled)\n",
    "    mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n",
    "    df_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\n",
    "    df_emb[\"label\"] = label\n",
    "    print(df_emb)\n",
    "    return df_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f42e1d-3953-4e07-94dd-2f612d30bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking embeddings of cat and car examples into one numpy array\n",
    "all_embeddings = np.concatenate([cat_embeddings, car_embeddings])\n",
    "\n",
    "# prepare labels for the 3 examples\n",
    "labels = ['cat'] * len(cat_embeddings) + ['car'] * len(car_embeddings)\n",
    "\n",
    "# compute dimensionality reduction for the 3 examples\n",
    "reduced_dim_emb = dimensionality_reduction(all_embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e2c347-584a-4bda-b45f-cf5cc76f6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot the centroids against the cluster\n",
    "fig, ax = plt.subplots(figsize=(8,6)) # Set figsize\n",
    "\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "sns.scatterplot(data=reduced_dim_emb, \n",
    "                x=reduced_dim_emb['X'], \n",
    "                y=reduced_dim_emb['Y'], \n",
    "                hue='label', \n",
    "                palette='bright')\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.title('Scatter plot of images of cats and cars using UMAP')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0aefcc",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>Note:</b>\n",
    "<br>\n",
    "- The embeddings of image-text pairs of `cats` (i.e., blue dots) are\n",
    "<br>\n",
    "closed to each other.\n",
    "<br>\n",
    "- The embeddings of image-text pairs of `cars` (i.e., orange dots) are\n",
    "<br>\n",
    "closed to each other.\n",
    "<br>\n",
    "- The embeddings of image-text pairs of `cats` (blue dots) are far away\n",
    "<br>\n",
    "from the embeddings of image-text pairs of `cars` (orange dots).\n",
    "<br>\n",
    "Note that UMAP includes some randomness, Your clusters may not appear the same as those in the videos however cats and cars should still be clustered separately.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6711033c",
   "metadata": {},
   "source": [
    "### Take-Home Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b65d17",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>Notes:</b>\n",
    "<br>\n",
    "- While we presented the two metrics: Cosine Similarity and \n",
    "<br>\n",
    "Euclidean Distance, and one visualization technique for embeddings:\n",
    "<br>\n",
    "UMAP above to demonstrate the meaning of embeddings, \n",
    "<br>\n",
    "you can also use other metrics (e.g., Cosine Distance and\n",
    "<br>\n",
    "Minkowski Distance) and other visualization techniques (e.g., t-SNE)\n",
    "<br>\n",
    "to verify the embeddings.\n",
    "<br>\n",
    "- There are other multimodal embedding models that can compute \n",
    "<br>\n",
    "the embeddings for images and texts like BridgeTower does. For example,\n",
    "<br>\n",
    "<a href=\"https://github.com/openai/CLIP\">CLIP</a> for image embedding and <a href=\"https://huggingface.co/sentence-transformers\">Sentence Transformer</a> for text embedding.\n",
    "\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
